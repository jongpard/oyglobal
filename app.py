# -*- coding: utf-8 -*-
"""
ì˜¬ë¦¬ë¸Œì˜ ê¸€ë¡œë²Œëª° ë² ìŠ¤íŠ¸ì…€ëŸ¬ ë­í‚¹ ìë™í™” (USD)
- ì†ŒìŠ¤: https://global.oliveyoung.com/display/page/best-seller?target=pillsTab1Nav1
- HTTP(ì •ì ) â†’ ë¶€ì¡± ì‹œ Playwright(ë™ì ) í´ë°±
- íŒŒì¼ëª…: ì˜¬ë¦¬ë¸Œì˜ê¸€ë¡œë²Œ_ë­í‚¹_YYYY-MM-DD.csv (KST)
- ì „ì¼ CSV ë¹„êµ Top30 â†’ Slack ì•Œë¦¼

í•„ìš” í™˜ê²½ë³€ìˆ˜:
  SLACK_WEBHOOK_URL
  GOOGLE_CLIENT_ID, GOOGLE_CLIENT_SECRET, GOOGLE_REFRESH_TOKEN
  GDRIVE_FOLDER_ID
  DRIVE_AUTH_MODE = oauth_only (ê¶Œì¥)
"""
import os, re, io, math, json, pytz, traceback
import datetime as dt
from dataclasses import dataclass
from typing import List, Dict, Optional, Tuple

import requests
import pandas as pd
from bs4 import BeautifulSoup

BEST_URL = "https://global.oliveyoung.com/display/page/best-seller?target=pillsTab1Nav1"
KST = pytz.timezone("Asia/Seoul")

# ---------- ì‹œê°„/ë¬¸ì ìœ í‹¸ ----------
def now_kst(): return dt.datetime.now(KST)
def today_kst_str(): return now_kst().strftime("%Y-%m-%d")
def yesterday_kst_str(): return (now_kst() - dt.timedelta(days=1)).strftime("%Y-%m-%d")
def build_filename(d): return f"ì˜¬ë¦¬ë¸Œì˜ê¸€ë¡œë²Œ_ë­í‚¹_{d}.csv"
def clean_text(s): return re.sub(r"\s+", " ", (s or "")).strip()
def to_float(s):
    if not s: return None
    m = re.findall(r"[\d]+(?:\.[\d]+)?", str(s)); return float(m[0]) if m else None

# ---------- ê°€ê²©/í‘œê¸° ìœ í‹¸ ----------
PRICE_RE = re.compile(r"(?:US\$|\$)\s*([\d.,]+)")
def parse_price_to_float(text: str) -> Optional[float]:
    if not text: return None
    t = text.replace("US$", "").replace("$", "").replace(",", "").strip()
    try: return float(t)
    except: return None

def fmt_currency_usd(v) -> str:
    try:
        if v is None or (isinstance(v, float) and math.isnan(v)): return "$0.00"
        return f"${float(v):,.2f}"
    except: return "$0.00"

def slack_escape(s): return s.replace("&","&amp;").replace("<","&lt;").replace(">","&gt;")

def make_display_name(brand: str, product: str, include_brand: bool) -> str:
    product = clean_text(product); brand = clean_text(brand)
    if not include_brand or not brand: return product
    if re.match(rf"^\[?\s*{re.escape(brand)}\b", product, flags=re.I): return product
    return f"{brand} {product}"

def discount_floor(orig: Optional[float], sale: Optional[float], percent_text: Optional[str]) -> Optional[int]:
    if percent_text:
        n = to_float(percent_text)
        if n is not None: return int(n // 1)
    if orig and sale and orig > 0:
        return max(0, int(math.floor((1 - sale / orig) * 100)))
    return None

# ---------- ë°ì´í„° ëª¨ë¸ ----------
@dataclass
class Product:
    rank: Optional[int]
    brand: str
    title: str   # ì œí’ˆëª… (ì‚¬ì´íŠ¸ í‘œê¸°)
    price: Optional[float]       # íŒë§¤ê°€
    orig_price: Optional[float]  # ì •ê°€(ì—†ìœ¼ë©´ íŒë§¤ê°€ì™€ ë™ì¼)
    discount_percent: Optional[int]
    url: str

# ---------- ì •ì  íŒŒì‹± ----------
def parse_static_html(html: str) -> List[Product]:
    soup = BeautifulSoup(html, "lxml")
    container = soup.select_one("#pillsTab1Nav1, [id*='pillsTab1Nav1']")
    root = container or soup
    cards = root.select("ul#orderBestProduct li.order-best-product.prdt-unit")
    items: List[Product] = []
    for idx, li in enumerate(cards, start=1):
        name = ""
        nm = li.select_one("dl.brand-info dd, .brand-info dd, .product_name, .name, .tit, .prd_name")
        if nm: name = clean_text(nm.get_text(" ", strip=True))

        brand = ""
        b = li.select_one("dl.brand-info dt, .brand, .brand_name, .brandName")
        if b: brand = clean_text(b.get_text(" ", strip=True))

        a = li.select_one("a[href]")
        link = a["href"] if (a and a.has_attr("href")) else ""
        if link.startswith("/"): link = "https://global.oliveyoung.com" + link

        rtxt = li.select_one(".rank-badge span, .rank-badge")
        rank = None
        if rtxt:
            n = to_float(clean_text(rtxt.get_text()))
            if n is not None: rank = int(n)
        if rank is None: rank = idx

        # ê°€ê²©: .price-info ì „ì²´ì—ì„œ ëª¨ë“  ë‹¬ëŸ¬ ê¸ˆì•¡ ì¶”ì¶œ â†’ sale=min, orig=max
        pbox = li.select_one(".price-info") or li
        ptxt = clean_text(pbox.get_text(" ", strip=True))
        nums = [parse_price_to_float(x) for x in PRICE_RE.findall(ptxt)]
        nums = [x for x in nums if x is not None]
        sale = orig = None
        if len(nums) == 1: sale = nums[0]
        elif len(nums) >= 2: sale, orig = min(nums), max(nums)

        # ë°±ì—… ì…€ë ‰í„°
        if sale is None:
            sale_el = li.select_one(".price-info .point, .price-info strong, .price-info .sale_price, .price-info .price")
            if sale_el: sale = parse_price_to_float(sale_el.get_text())
        if orig is None:
            orig_el = li.select_one(".price-info span, .price-info del")
            if orig_el: orig = parse_price_to_float(orig_el.get_text())
        if sale is None and orig is not None:
            sale = orig

        pct_txt = ""
        pct_el = li.select_one(".price-info .rate, .discount-rate, .percent, .dc")
        if pct_el: pct_txt = clean_text(pct_el.get_text())
        pct = discount_floor(orig, sale, pct_txt)

        if name and link:
            items.append(Product(rank, brand, name, sale, orig, pct, link))
    return items

def fetch_by_http() -> List[Product]:
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36",
        "Accept-Language": "en-US,en;q=0.9",
        "Cache-Control": "no-cache", "Pragma": "no-cache",
    }
    r = requests.get(BEST_URL, headers=headers, timeout=25)
    r.raise_for_status()
    return parse_static_html(r.text)

# ---------- Playwright(ê°•í™”): Global ì „í™˜ + í´ë§ ëŒ€ê¸° ----------
def fetch_by_playwright() -> List[Product]:
    from playwright.sync_api import sync_playwright
    import time, pathlib

    CARD_SELS = [
        "ul#orderBestProduct li.order-best-product.prdt-unit",
        "#pillsTab1Nav1 ul#orderBestProduct li.order-best-product.prdt-unit",
        "#pillsTab1Nav1 li.order-best-product.prdt-unit",
    ]

    def _debug_dump(page, tag="global"):
        pathlib.Path("data/debug").mkdir(parents=True, exist_ok=True)
        with open(f"data/debug/page_{tag}.html", "w", encoding="utf-8") as f:
            f.write(page.content())
        page.screenshot(path=f"data/debug/page_{tag}.png", full_page=True)

    def _force_region_global(page):
        # í—¤ë” ìš°ì¸¡ ì§€ì—­ ë“œë¡­ë‹¤ìš´(select ë˜ëŠ” ì»¤ìŠ¤í…€)
        try:
            sel = page.locator("select.cntry-select-box").first
            if sel.count():
                sel.select_option(label="Global")
                page.wait_for_timeout(500)
                return
        except: pass
        # ì»¤ìŠ¤í…€ ì…€ë ‰í„°(USA â†’ Global)
        for open_sel in [
            ".cntry-select-box-wrapper .selected-cntry",
            "button[aria-haspopup='listbox']",
            "button:has-text('USA')",
            "[role='button']:has-text('USA')"
        ]:
            try:
                page.locator(open_sel).first.click(timeout=1200)
                for opt in ["li[role='option']:has-text('Global')", "li:has-text('Global')", "text=Global"]:
                    try:
                        page.locator(opt).first.click(timeout=1200)
                        return
                    except: pass
            except: pass
        # ë‚´ë¶€ íƒ­ë„ Global(Top Orders)ë¡œ í´ë¦­
        for tab_sel in ["[href*='pillsTab1Nav1']", "#pillsTab1Nav1-tab", "[data-bs-target='#pillsTab1Nav1']",
                        "button:has-text('Top Orders')"]:
            try:
                page.locator(tab_sel).first.click(timeout=1200)
                return
            except: pass

    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=True,
            args=["--disable-blink-features=AutomationControlled","--no-sandbox","--disable-dev-shm-usage"],
        )
        context = browser.new_context(
            viewport={"width":1366,"height":900},
            locale="en-US",
            timezone_id="Asia/Seoul",
            user_agent=("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123 Safari/537.36"),
            extra_http_headers={"Accept-Language":"en-US,en;q=0.9"},
        )
        context.add_init_script("Object.defineProperty(navigator,'webdriver',{get:()=>undefined});")

        page = context.new_page()
        page.goto(BEST_URL, wait_until="domcontentloaded", timeout=60_000)
        try: page.wait_for_load_state("networkidle", timeout=30_000)
        except: pass

        # ì¿ í‚¤/ë°°ë„ˆ ë‹«ê¸°
        for sel in ["#onetrust-accept-btn-handler","button:has-text('Accept')","button:has-text('í™•ì¸')","[aria-label='Close']"]:
            try: page.locator(sel).first.click(timeout=1200)
            except: pass

        _force_region_global(page)

        # ìŠ¤í¬ë¡¤ + í´ë§ (ìµœëŒ€ 35s)
        start = time.time(); found = 0
        while time.time() - start < 35:
            try: page.mouse.wheel(0, 1400)
            except: pass
            for sel in CARD_SELS:
                try: found = page.eval_on_selector_all(sel, "els => els.length")
                except: found = 0
                if found and found >= 10: break
            if found and found >= 10: break
            page.wait_for_timeout(800)

        if not (found and found >= 10):
            _debug_dump(page, "global_empty")
            context.close(); browser.close()
            return []

        # JSë¡œ í•„ìš”í•œ í•„ë“œë§Œ ì¶”ì¶œ
        data = page.evaluate("""
            (sels) => {
              const get = (el, s) => (el.querySelector(s)?.textContent || '').replace(/\\s+/g,' ').trim();
              const numsFrom = (t) => Array.from((t||'').matchAll(/(?:US\\$|\\$)\\s*([\\d.,]+)/g))
                                           .map(m => parseFloat(m[1].replace(/,/g,'')))
                                           .filter(v=>!isNaN(v));
              let nodes = [];
              for (const s of sels) { nodes = Array.from(document.querySelectorAll(s)); if (nodes.length >= 10) break; }
              return nodes.map((el, i) => {
                const brand = get(el, "dl.brand-info dt, .brand, .brand_name, .brandName");
                const name  = get(el, "dl.brand-info dd, .prd_name, .name, .product_name");
                const a     = el.querySelector("a[href]");
                const link  = a ? a.href : '';
                const rtxt  = get(el, ".rank-badge span, .rank-badge");
                const rank  = parseInt((rtxt||'').replace(/[^0-9]/g,'')) || (i+1);

                const pbox  = el.querySelector(".price-info") || el;
                const ptxt  = (pbox.textContent || '').replace(/\\s+/g,' ').trim();
                const arr   = numsFrom(ptxt);
                let sale=null, orig=null;
                if (arr.length===1){ sale=arr[0]; }
                else if (arr.length>=2){ sale=Math.min(...arr); orig=Math.max(...arr); }

                if (sale==null) sale = parseFloat((get(el, ".price-info .point, .price-info strong, .price-info .sale_price, .price-info .price")||'').replace(/[^\\d.]/g,''))||null;
                if (orig==null) orig = parseFloat((get(el, ".price-info span, .price-info del")||'').replace(/[^\\d.]/g,''))||null;
                if (sale==null && orig!=null) sale = orig;

                const pctTxt = get(el, ".price-info .rate, .discount-rate, .percent, .dc");
                return {rank, brand, name, link, sale, orig, pctTxt};
              }).filter(x => x.name && x.link);
            }
        """, CARD_SELS)

        context.close(); browser.close()

    items: List[Product] = []
    for r in data:
        items.append(Product(
            rank=int(r["rank"]),
            brand=clean_text(r["brand"]),
            title=clean_text(r["name"]),
            price=r["sale"],
            orig_price=r["orig"],
            discount_percent=discount_floor(r["orig"], r["sale"], r["pctTxt"]),
            url=r["link"],
        ))
    return items

def fetch_products() -> List[Product]:
    try:
        items = fetch_by_http()
        if len(items) >= 10: return items
    except Exception as e:
        print("[HTTP ì˜¤ë¥˜] â†’ Playwright í´ë°±:", e)
    return fetch_by_playwright()

# ---------- Google Drive (êµ­ë‚´íŒ ìŠ¤íƒ€ì¼: ë‹¨ìˆœ/ì•ˆì •) ----------
def normalize_folder_id(raw: str) -> str:
    """URL/ê³µë°±/ì¿¼ë¦¬ìŠ¤íŠ¸ë§ì´ ë“¤ì–´ì™€ë„ ìˆœìˆ˜ folderIdë§Œ ì¶”ì¶œ"""
    if not raw: return ""
    s = raw.strip()
    m = re.search(r"/folders/([a-zA-Z0-9_-]{10,})", s) or re.search(r"[?&]id=([a-zA-Z0-9_-]{10,})", s)
    return (m.group(1) if m else s)

def build_drive_service():
    """
    OAuth-only. ìŠ¤ì½”í”„ ë¯¸ì§€ì •(ê¸°ì¡´ í† í° ìŠ¤ì½”í”„ ì‚¬ìš©)ìœ¼ë¡œ invalid_scope íšŒí”¼.
    """
    from googleapiclient.discovery import build
    from google.oauth2.credentials import Credentials

    cid  = os.getenv("GOOGLE_CLIENT_ID")
    csec = os.getenv("GOOGLE_CLIENT_SECRET")
    rtk  = os.getenv("GOOGLE_REFRESH_TOKEN")
    if not (cid and csec and rtk):
        raise RuntimeError("OAuth ìê²©ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. GOOGLE_CLIENT_ID/SECRET/REFRESH_TOKEN í™•ì¸")

    creds = Credentials(
        None,
        refresh_token=rtk,
        token_uri="https://oauth2.googleapis.com/token",
        client_id=cid,
        client_secret=csec,
        # scopes=None  # â† ì˜ë„ì ìœ¼ë¡œ ì§€ì •í•˜ì§€ ì•ŠìŒ
    )
    svc = build("drive", "v3", credentials=creds, cache_discovery=False)
    try:
        about = svc.about().get(fields="user(displayName,emailAddress)").execute()
        u = about.get("user", {})
        print(f"[Drive] user={u.get('displayName')} <{u.get('emailAddress')}>")
    except Exception as e:
        print("[Drive] whoami ì‹¤íŒ¨:", e)
    return svc

def drive_upload_csv(service, folder_id: str, name: str, df: pd.DataFrame) -> str:
    from googleapiclient.http import MediaIoBaseUpload
    # ë™ì¼ íŒŒì¼ëª… ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸, ì—†ìœ¼ë©´ ìƒì„±
    q = f"name = '{name}' and '{folder_id}' in parents and trashed = false"
    res = service.files().list(q=q, fields="files(id,name)",
                               supportsAllDrives=True, includeItemsFromAllDrives=True).execute()
    file_id = res.get("files", [{}])[0].get("id") if res.get("files") else None

    buf = io.BytesIO(); df.to_csv(buf, index=False, encoding="utf-8-sig"); buf.seek(0)
    media = MediaIoBaseUpload(buf, mimetype="text/csv", resumable=False)

    if file_id:
        service.files().update(fileId=file_id, media_body=media, supportsAllDrives=True).execute()
        return file_id

    meta = {"name": name, "parents": [folder_id], "mimeType": "text/csv"}
    created = service.files().create(body=meta, media_body=media, fields="id",
                                     supportsAllDrives=True).execute()
    return created["id"]

def drive_download_csv(service, folder_id: str, name: str) -> Optional[pd.DataFrame]:
    from googleapiclient.http import MediaIoBaseDownload
    res = service.files().list(q=f"name = '{name}' and '{folder_id}' in parents and trashed = false",
                               fields="files(id,name)", supportsAllDrives=True,
                               includeItemsFromAllDrives=True).execute()
    files = res.get("files", [])
    if not files: return None
    fid = files[0]["id"]
    req = service.files().get_media(fileId=fid, supportsAllDrives=True)
    fh = io.BytesIO(); dl = MediaIoBaseDownload(fh, req); done=False
    while not done: _, done = dl.next_chunk()
    fh.seek(0); return pd.read_csv(fh)

# ---------- Slack ----------
def slack_post(text: str):
    url = os.getenv("SLACK_WEBHOOK_URL")
    if not url:
        print("[ê²½ê³ ] SLACK_WEBHOOK_URL ë¯¸ì„¤ì • â†’ ì½˜ì†” ì¶œë ¥\n", text); return
    r = requests.post(url, json={"text": text}, timeout=20)
    if r.status_code >= 300:
        print("[Slack ì‹¤íŒ¨]", r.status_code, r.text)

# ---------- ë¹„êµ/ë©”ì‹œì§€ ----------
def to_dataframe(products: List[Product], date_str: str) -> pd.DataFrame:
    return pd.DataFrame([{
        "date": date_str,
        "rank": p.rank,
        "brand": p.brand,
        "product_name": p.title,
        "price": p.price,
        "orig_price": p.orig_price,
        "discount_percent": p.discount_percent,
        "url": p.url,
    } for p in products])

def line_move(name_link: str, prev_rank: Optional[int], curr_rank: Optional[int]) -> Tuple[str, int]:
    if prev_rank is None and curr_rank is not None: return f"- {name_link} NEW â†’ {curr_rank}ìœ„", 99999
    if curr_rank is None and prev_rank is not None: return f"- {name_link} {prev_rank}ìœ„ â†’ OUT", 99999
    if prev_rank is None or curr_rank is None:    return f"- {name_link}", 0
    delta = prev_rank - curr_rank
    if   delta > 0: return f"- {name_link} {prev_rank}ìœ„ â†’ {curr_rank}ìœ„ (â†‘{delta})", delta
    elif delta < 0: return f"- {name_link} {prev_rank}ìœ„ â†’ {curr_rank}ìœ„ (â†“{abs(delta)})", abs(delta)
    else:           return f"- {name_link} {prev_rank}ìœ„ â†’ {curr_rank}ìœ„ (ë³€ë™ì—†ìŒ)", 0

def build_sections(df_today: pd.DataFrame, df_prev: Optional[pd.DataFrame]) -> Dict[str, List[str]]:
    S = {"top10": [], "rising": [], "newcomers": [], "falling": [], "outs": [], "inout_count": 0}

    # TOP10 (ë¸Œëœë“œ í¬í•¨)
    top10 = df_today.dropna(subset=["rank"]).sort_values("rank").head(10)
    for _, r in top10.iterrows():
        disp = make_display_name(r.get("brand",""), r["product_name"], include_brand=True)
        name_link = f"<{r['url']}|{slack_escape(disp)}>"
        price_txt = fmt_currency_usd(r["price"])
        dc = r.get("discount_percent"); tail = f" (â†“{int(dc)}%)" if pd.notnull(dc) else ""
        S["top10"].append(f"{int(r['rank'])}. {name_link} â€” {price_txt}{tail}")

    # ì „ì¼ CSV ì—†ìœ¼ë©´ ë¹„êµ ì„¹ì…˜ ìƒëµ
    if df_prev is None or not len(df_prev):
        return S

    df_t = df_today.copy(); df_t["key"] = df_t["url"]; df_t.set_index("key", inplace=True)
    df_p = df_prev.copy(); df_p["key"] = df_p["url"]; df_p.set_index("key", inplace=True)

    t30 = df_t[(df_t["rank"].notna()) & (df_t["rank"] <= 30)].copy()
    p30 = df_p[(df_p["rank"].notna()) & (df_p["rank"] <= 30)].copy()
    common = set(t30.index) & set(p30.index)
    new    = set(t30.index) - set(p30.index)
    out    = set(p30.index) - set(t30.index)

    def full_name_link(row):
        disp = make_display_name(row.get("brand",""), row.get("product_name",""), include_brand=True)
        return f"<{row['url']}|{slack_escape(disp)}>"

    # ğŸ”¥ ê¸‰ìƒìŠ¹ (ìƒìœ„ 3)
    rising = []
    for k in common:
        prev_rank = int(p30.loc[k,"rank"]); curr_rank = int(t30.loc[k,"rank"])
        imp = prev_rank - curr_rank
        if imp > 0:
            line,_ = line_move(full_name_link(t30.loc[k]), prev_rank, curr_rank)
            rising.append((imp, curr_rank, prev_rank, slack_escape(t30.loc[k].get("product_name","")), line))
    rising.sort(key=lambda x: (-x[0], x[1], x[2], x[3]))
    S["rising"] = [e[-1] for e in rising[:3]]

    # ğŸ†• ë‰´ë­ì»¤ (â‰¤3)
    newcomers = []
    for k in new:
        curr_rank = int(t30.loc[k,"rank"])
        newcomers.append((curr_rank, f"- {full_name_link(t30.loc[k])} NEW â†’ {curr_rank}ìœ„"))
    newcomers.sort(key=lambda x: x[0])
    S["newcomers"] = [line for _, line in newcomers[:3]]

    # ğŸ“‰ ê¸‰í•˜ë½ (ìƒìœ„ 5)
    falling = []
    for k in common:
        prev_rank = int(p30.loc[k,"rank"]); curr_rank = int(t30.loc[k,"rank"])
        drop = curr_rank - prev_rank
        if drop > 0:
            line,_ = line_move(full_name_link(t30.loc[k]), prev_rank, curr_rank)
            falling.append((drop, curr_rank, prev_rank, slack_escape(t30.loc[k].get("product_name","")), line))
    falling.sort(key=lambda x: (-x[0], x[1], x[2], x[3]))
    S["falling"] = [e[-1] for e in falling[:5]]

    # OUT (ê¸‰í•˜ë½ ì„¹ì…˜ ì•„ë˜)
    for k in sorted(list(out)):
        prev_rank = int(p30.loc[k,"rank"])
        line,_ = line_move(full_name_link(p30.loc[k]), prev_rank, None)
        S["outs"].append(line)

    S["inout_count"] = len(new) + len(out)
    return S

def build_slack_message(date_str: str, S: Dict[str, List[str]]) -> str:
    lines: List[str] = []
    lines.append(f"*ì˜¬ë¦¬ë¸Œì˜ ê¸€ë¡œë²Œ ì „ì²´ ë­í‚¹ 100 â€” {date_str}*")
    lines.append("")
    lines.append("*TOP 10*");          lines.extend(S.get("top10") or ["- ë°ì´í„° ì—†ìŒ"]); lines.append("")
    lines.append("*ğŸ”¥ ê¸‰ìƒìŠ¹*");       lines.extend(S.get("rising") or ["- í•´ë‹¹ ì—†ìŒ"]); lines.append("")
    lines.append("*ğŸ†• ë‰´ë­ì»¤*");       lines.extend(S.get("newcomers") or ["- í•´ë‹¹ ì—†ìŒ"]); lines.append("")
    lines.append("*ğŸ“‰ ê¸‰í•˜ë½*");       lines.extend(S.get("falling") or ["- í•´ë‹¹ ì—†ìŒ"])
    lines.extend(S.get("outs") or [])
    lines.append(""); lines.append("*ğŸ”„ ë­í¬ ì¸&ì•„ì›ƒ*")
    lines.append(f"{S.get('inout_count', 0)}ê°œì˜ ì œí’ˆì´ ì¸&ì•„ì›ƒ ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return "\n".join(lines)

# ---------- ë©”ì¸ ----------
def main():
    date_str = today_kst_str()
    ymd_yesterday = yesterday_kst_str()
    file_today = build_filename(date_str)
    file_yesterday = build_filename(ymd_yesterday)

    print("ìˆ˜ì§‘ ì‹œì‘:", BEST_URL)
    items: List[Product] = []
    try:
        items = fetch_by_http(); print(f"[HTTP] ìˆ˜ì§‘: {len(items)}ê°œ")
    except Exception as e:
        print("[HTTP ì˜¤ë¥˜]", e)
    if len(items) < 10:
        print("[Playwright í´ë°± ì§„ì…]")
        items = fetch_by_playwright()
    print("ìˆ˜ì§‘ ì™„ë£Œ:", len(items))
    if len(items) < 10:
        raise RuntimeError("ì œí’ˆ ì¹´ë“œê°€ ë„ˆë¬´ ì ê²Œ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤. ì…€ë ‰í„°/ë Œë”ë§ ì ê²€ í•„ìš”")

    df_today = to_dataframe(items, date_str)
    os.makedirs("data", exist_ok=True)
    df_today.to_csv(os.path.join("data", file_today), index=False, encoding="utf-8-sig")
    print("ë¡œì»¬ ì €ì¥:", file_today)

    # --- Drive ì—…ë¡œë“œ + ì „ì¼ CSV ë¡œë“œ (êµ­ë‚´íŒ ìŠ¤íƒ€ì¼) ---
    df_prev = None
    raw_folder = os.getenv("GDRIVE_FOLDER_ID", "")
    folder = normalize_folder_id(raw_folder)

    if folder:
        try:
            mask = folder[:4] + "â€¦" + folder[-4:]
            print(f"[Drive] folder_id={mask} (normalized)")
            svc = build_drive_service()

            # ê³§ë°”ë¡œ ì—…ë¡œë“œ/ë‹¤ìš´ë¡œë“œ (í”„ë¦¬í”Œë¼ì´íŠ¸ ì—†ìŒ)
            drive_upload_csv(svc, folder, file_today, df_today)
            print("Google Drive ì—…ë¡œë“œ ì™„ë£Œ:", file_today)

            df_prev = drive_download_csv(svc, folder, file_yesterday)
            print("ì „ì¼ CSV", "ì„±ê³µ" if df_prev is not None else "ë¯¸ë°œê²¬")
        except Exception as e:
            print("Google Drive ì²˜ë¦¬ ì˜¤ë¥˜:", e)
            traceback.print_exc()
    else:
        print("[ê²½ê³ ] GDRIVE_FOLDER_ID ë¯¸ì„¤ì • â†’ ë“œë¼ì´ë¸Œ ì—…ë¡œë“œ/ì „ì¼ ë¹„êµ ìƒëµ")

    S = build_sections(df_today, df_prev)
    msg = build_slack_message(date_str, S)
    slack_post(msg)
    print("Slack ì „ì†¡ ì™„ë£Œ")

if __name__ == "__main__":
    try: main()
    except Exception as e:
        print("[ì˜¤ë¥˜ ë°œìƒ]", e); traceback.print_exc()
        try: slack_post(f"*ì˜¬ë¦¬ë¸Œì˜ ê¸€ë¡œë²Œëª° ë­í‚¹ ìë™í™” ì‹¤íŒ¨*\n```\n{e}\n```")
        except: pass
        raise
