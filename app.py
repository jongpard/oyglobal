# -*- coding: utf-8 -*-
"""
ì˜¬ë¦¬ë¸Œì˜ ê¸€ë¡œë²Œëª° ë² ìŠ¤íŠ¸ì…€ëŸ¬ ë­í‚¹ ìˆ˜ì§‘/ë¹„êµ/ì•Œë¦¼ (USD)
- ë°ì´í„° ì†ŒìŠ¤: https://global.oliveyoung.com/display/page/best-seller?target=pillsTab1Nav1
- HTTP ìš°ì„  â†’ ê²°ê³¼ ë¶€ì¡± ì‹œ Playwright í´ë°±
- ì €ì¥ íŒŒì¼ëª…: ì˜¬ë¦¬ë¸Œì˜ê¸€ë¡œë²Œ_ë­í‚¹_YYYY-MM-DD.csv (KST ê¸°ì¤€)
- ì „ì¼ CSVì™€ ë¹„êµí•˜ì—¬ TOP10/ê¸‰ìƒìŠ¹/ë‰´ë­ì»¤/ê¸‰í•˜ë½(OUT í¬í•¨)/ë­í¬ ì¸&ì•„ì›ƒ ê³„ì‚°
- Slack ë©”ì‹œì§€: êµ­ë‚´ ë²„ì „ê³¼ ë™ì¼í•œ êµ¬ì¡°/í¬ë§·(ëª¨ë“  ì œëª©/ì†Œì œëª© êµµê²Œ)
- Google Drive ì—…ë¡œë“œ/ì „ì¼ íŒŒì¼ ì¡°íšŒ (ì„œë¹„ìŠ¤ê³„ì • or OAuth RefreshToken ëª¨ë‘ ì§€ì›)
- í™˜ê²½ë³€ìˆ˜:
  SLACK_WEBHOOK_URL
  GOOGLE_CLIENT_ID, GOOGLE_CLIENT_SECRET, GOOGLE_REFRESH_TOKEN (OAuth ì‚¬ìš© ì‹œ)
  GDRIVE_FOLDER_ID
  GDRIVE_SERVICE_ACCOUNT_JSON (ì„œë¹„ìŠ¤ê³„ì • JSON ë¬¸ìì—´; ìˆìœ¼ë©´ ì´ê±¸ ìš°ì„  ì‚¬ìš©)
"""
import os
import re
import io
import math
import json
import time
import pytz
import uuid
import traceback
import datetime as dt
from dataclasses import dataclass
from typing import List, Dict, Optional, Tuple

import requests
import pandas as pd
from bs4 import BeautifulSoup

# ==== í™˜ê²½ ìƒìˆ˜ ====
BEST_URL = "https://global.oliveyoung.com/display/page/best-seller?target=pillsTab1Nav1"
KST = pytz.timezone("Asia/Seoul")

# ==== ìœ í‹¸ ====
def now_kst() -> dt.datetime:
    return dt.datetime.now(KST)

def today_kst_str() -> str:
    return now_kst().strftime("%Y-%m-%d")

def yesterday_kst_str() -> str:
    y = now_kst() - dt.timedelta(days=1)
    return y.strftime("%Y-%m-%d")

def build_filename(date_str: str) -> str:
    return f"ì˜¬ë¦¬ë¸Œì˜ê¸€ë¡œë²Œ_ë­í‚¹_{date_str}.csv"

def to_float(s: Optional[str]) -> Optional[float]:
    if not s:
        return None
    m = re.findall(r"[\d]+(?:\.[\d]+)?", str(s))
    if not m:
        return None
    try:
        return float(m[0])
    except:
        return None

def extract_percent_floor(orig_price: Optional[float], sale_price: Optional[float], percent_text: Optional[str]) -> Optional[int]:
    """
    ìš°ì„ ìˆœìœ„:
      1) percent_textì—ì„œ ìˆ«ì ì¶”ì¶œí•´ ë‚´ë¦¼
      2) ì›ê°€/íŒë§¤ê°€ê°€ ìˆìœ¼ë©´ ê³„ì‚°í•´ ë‚´ë¦¼
    """
    # 1) ì§ì ‘ í‘œê¸°ëœ í¼ì„¼íŠ¸
    if percent_text:
        n = to_float(percent_text)
        if n is not None:
            return int(math.floor(n))
    # 2) ê°€ê²©ìœ¼ë¡œ ê³„ì‚°
    if orig_price and sale_price and orig_price > 0:
        pct = (1 - (sale_price / orig_price)) * 100.0
        return max(0, int(math.floor(pct)))
    return None

def clean_text(s: Optional[str]) -> str:
    return re.sub(r"\s+", " ", (s or "")).strip()

def remove_brand_from_title(title: str, brand: str) -> str:
    """TOP10 í‘œì‹œì— ë¸Œëœë“œ ì œê±° (êµ­ë‚´ ë²„ì „ ê·œì¹™ ë™ì¼)"""
    t = clean_text(title)
    b = clean_text(brand)
    if not b:
        return t
    # [ë¸Œëœë“œ] ì œí’ˆëª… / ë¸Œëœë“œ ì œí’ˆëª… / (ë¸Œëœë“œ) ì œí’ˆëª… ë“± ì„ ë‘ë¶€ ì œê±°
    patterns = [
        rf"^\[?\s*{re.escape(b)}\s*\]?\s*[-â€“â€”:|]*\s*",
        rf"^\(?\s*{re.escape(b)}\s*\)?\s*[-â€“â€”:|]*\s*",
    ]
    for pat in patterns:
        t2 = re.sub(pat, "", t, flags=re.I)
        if t2 != t:
            t = t2.strip()
            break
    return t

def slack_escape(s: str) -> str:
    # Slackì˜ ë§í¬ í…ìŠ¤íŠ¸ì—ëŠ” &, <, > ì´ìŠˆë§Œ ê°„ë‹¨íˆ ì²˜ë¦¬
    return s.replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")

def fmt_currency_usd(v: Optional[float]) -> str:
    if v is None:
        return "$0.00"
    return f"${v:,.2f}"

@dataclass
class Product:
    rank: Optional[int]             # 1-based
    brand: str
    title: str
    price: Optional[float]          # íŒë§¤ê°€(í• ì¸ê°€)
    orig_price: Optional[float]     # ì •ìƒê°€(ìˆìœ¼ë©´)
    discount_percent: Optional[int] # ì†Œìˆ˜ì  ì—†ì´ ë²„ë¦¼
    url: str

def parse_cards_from_html(html: str) -> List[Product]:
    """
    HTTP ì‘ë‹µì˜ HTMLì—ì„œ ìµœëŒ€í•œ íŒŒì‹± (ì „ë©´ JSì¼ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ ì‹¤íŒ¨ ê°€ëŠ¥)
    ë‹¤ì–‘í•œ í´ë˜ìŠ¤/êµ¬ì¡°ë¥¼ í¬ê´„ì ìœ¼ë¡œ ì‹œë„
    """
    soup = BeautifulSoup(html, "lxml")

    # í›„ë³´ ì…€ë ‰í„°ë“¤ (ìƒí™© ë³€í™” ëŒ€ì‘)
    item_selectors = [
        "ul.tab_cont_list li",
        "ul.best_list li",
        "ul#bestSellerContent li",
        "ul li.prod_item",
        "ul li",
        "div.prod_area",
    ]

    name_selectors = [".product_name", ".prod_name", ".name", ".tit", ".tx_name", ".item_name", "a[title]"]
    brand_selectors = [".brand", ".brand_name", ".tx_brand", ".brandName"]
    link_selectors = ["a", "a.prod_link", "a.link", "a.detail_link"]
    price_selectors = [".price .num", ".sale_price", ".discount_price", ".final_price", ".price", ".value"]
    orig_price_selectors = [".orig_price", ".normal_price", ".consumer", ".strike", ".was"]
    percent_selectors = [".percent", ".dc", ".discount_rate", ".rate"]

    def pick_text(el, selectors):
        for sel in selectors:
            node = el.select_one(sel)
            if node:
                t = clean_text(node.get_text(" ", strip=True))
                if t:
                    return t
        return ""

    def pick_link(el, selectors):
        for sel in selectors:
            a = el.select_one(sel)
            if a and a.has_attr("href"):
                href = a["href"].strip()
                if href and not href.startswith("javascript"):
                    return href
        return ""

    items: List[Product] = []
    found = []
    for sel in item_selectors:
        found = soup.select(sel)
        if found and len(found) >= 10:
            break

    # ìˆœìœ„ëŠ” DOM ìˆœì„œë¡œ ë§¤ê¸°ë˜, ì¹´ë“œê°€ ë¶€ì¡±í•˜ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ìœ ì§€ (í´ë°± ìœ ë„)
    for idx, li in enumerate(found, start=1):
        title = pick_text(li, name_selectors)
        brand = pick_text(li, brand_selectors)
        link = pick_link(li, link_selectors)
        price_txt = pick_text(li, price_selectors)
        orig_txt = pick_text(li, orig_price_selectors)
        pct_txt = pick_text(li, percent_selectors)

        sale = to_float(price_txt)
        orig = to_float(orig_txt)
        pct = extract_percent_floor(orig, sale, pct_txt)

        # ìƒëŒ€ê²½ë¡œ ë³´ì •
        if link and link.startswith("/"):
            link = "https://global.oliveyoung.com" + link

        if title and link:
            items.append(Product(
                rank=idx,
                brand=brand,
                title=title,
                price=sale,
                orig_price=orig,
                discount_percent=pct,
                url=link
            ))

    return items

def fetch_by_http() -> List[Product]:
    hdrs = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0.0.0 Safari/537.36"
        ),
        # ë¯¸êµ­ ì‚¬ìš©ìë¡œ ìœ„ì¥
        "Accept-Language": "en-US,en;q=0.9",
        "Cache-Control": "no-cache",
        "Pragma": "no-cache",
    }
    r = requests.get(BEST_URL, headers=hdrs, timeout=20)
    r.raise_for_status()
    return parse_cards_from_html(r.text)

def fetch_by_playwright() -> List[Product]:
    """
    Playwright í´ë°±. ë‹¤ì–‘í•œ ì…€ë ‰í„° ì¡°í•©ì„ ì‚¬ìš©í•˜ì—¬ ì¹´ë“œ ìš”ì†Œë¥¼ ìˆ˜ì§‘.
    - locale='en-US', timezone='America/Los_Angeles' ë¡œ ê¸€ë¡œë²Œëª°/ë‹¬ëŸ¬ ìœ ë„
    """
    from playwright.sync_api import sync_playwright

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(
            locale="en-US",
            timezone_id="America/Los_Angeles",
            user_agent=(
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/120.0.0.0 Safari/537.36"
            ),
        )
        page = context.new_page()
        page.goto(BEST_URL, wait_until="networkidle", timeout=60_000)

        # ë™ì  ë¡œë“œ ëŒ€ê¸°: ì—¬ëŸ¬ í›„ë³´ ì…€ë ‰í„° ì¤‘ í•˜ë‚˜ê°€ ë‚˜íƒ€ë‚˜ë©´ OK
        candidates = [
            "ul.tab_cont_list li",
            "ul#bestSellerContent li",
            "ul.best_list li",
            "div.best_seller_wrap li",
            "li .product_name",
        ]

        success = False
        for sel in candidates:
            try:
                page.wait_for_selector(sel, timeout=30_000)
                success = True
                break
            except:
                pass
        if not success:
            # í•œ ë²ˆ ë” ìŠ¤í¬ë¡¤ ìœ ë„
            page.evaluate("() => window.scrollTo(0, document.body.scrollHeight)")
            page.wait_for_timeout(1500)
            page.wait_for_selector(candidates[-1], timeout=30_000)

        # ë¸Œë¼ìš°ì € ë‚´ì—ì„œ ë„“ê²Œ ê¸ì–´ì˜¤ê¸° (ë‹¤ì–‘í•œ í´ë˜ìŠ¤ ëŒ€ì‘)
        data = page.evaluate(
            """
            () => {
              const pick = (el, sels) => {
                for (const s of sels) {
                  const x = el.querySelector(s);
                  if (x) {
                    const t = (x.textContent || '').trim();
                    if (t) return t;
                  }
                }
                return '';
              };
              const pickLink = (el, sels) => {
                for (const s of sels) {
                  const a = el.querySelector(s);
                  if (a && a.href) return a.href;
                }
                return '';
              };
              const itemSelectors = [
                'ul.tab_cont_list li',
                'ul#bestSellerContent li',
                'ul.best_list li',
                'ul li.prod_item',
                'ul li',
                'div.prod_area',
              ];
              const nameSelectors = ['.product_name', '.prod_name', '.name', '.tit', '.tx_name', '.item_name', 'a[title]'];
              const brandSelectors = ['.brand', '.brand_name', '.tx_brand', '.brandName'];
              const linkSelectors = ['a', 'a.prod_link', 'a.link', 'a.detail_link'];
              const priceSelectors = ['.price .num', '.sale_price', '.discount_price', '.final_price', '.price', '.value'];
              const origSelectors  = ['.orig_price', '.normal_price', '.consumer', '.strike', '.was'];
              const percentSelectors = ['.percent', '.dc', '.discount_rate', '.rate'];

              let nodes = [];
              for (const s of itemSelectors) {
                const found = Array.from(document.querySelectorAll(s));
                if (found.length >= 10) { nodes = found; break; }
                if (!nodes.length) nodes = found;
              }
              return nodes.map((el, idx) => {
                const title = pick(el, nameSelectors);
                const brand = pick(el, brandSelectors);
                const link = pickLink(el, linkSelectors);
                const price = pick(el, priceSelectors);
                const orig  = pick(el, origSelectors);
                const pct   = pick(el, percentSelectors);
                return {rank: idx + 1, title, brand, link, price, orig, pct};
              }).filter(x => x.title && x.link);
            }
            """
        )
        context.close()
        browser.close()

    products: List[Product] = []
    for row in data:
        sale = to_float(row.get("price"))
        orig = to_float(row.get("orig"))
        pct = extract_percent_floor(orig, sale, row.get("pct"))

        products.append(Product(
            rank=row.get("rank"),
            brand=clean_text(row.get("brand")),
            title=clean_text(row.get("title")),
            price=sale,
            orig_price=orig,
            discount_percent=pct,
            url=row.get("link"),
        ))
    return products

def fetch_products() -> List[Product]:
    # 1) HTTP ì‹œë„
    try:
        items = fetch_by_http()
        if len(items) >= 20:
            return items
    except Exception as e:
        print("[HTTP] ì‹¤íŒ¨/ë¶€ì¡± â†’ Playwright í´ë°±:", e)

    # 2) Playwright í´ë°±
    items = fetch_by_playwright()
    return items

# ==== Google Drive ====
def build_drive_service():
    from googleapiclient.discovery import build
    from google.oauth2 import service_account
    from google.oauth2.credentials import Credentials

    sa_json = os.getenv("GDRIVE_SERVICE_ACCOUNT_JSON", "").strip()
    scopes = ["https://www.googleapis.com/auth/drive"]
    creds = None

    if sa_json:
        # ì„œë¹„ìŠ¤ê³„ì • ìš°ì„ 
        info = json.loads(sa_json)
        creds = service_account.Credentials.from_service_account_info(info, scopes=scopes)
    else:
        # OAuth Refresh Token
        client_id = os.getenv("GOOGLE_CLIENT_ID")
        client_secret = os.getenv("GOOGLE_CLIENT_SECRET")
        refresh_token = os.getenv("GOOGLE_REFRESH_TOKEN")
        if not (client_id and client_secret and refresh_token):
            raise RuntimeError("Google Drive ìê²©ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        creds = Credentials(
            None,
            refresh_token=refresh_token,
            token_uri="https://oauth2.googleapis.com/token",
            client_id=client_id,
            client_secret=client_secret,
            scopes=scopes,
        )
    return build("drive", "v3", credentials=creds, cache_discovery=False)

def drive_upload_csv(service, folder_id: str, name: str, df: pd.DataFrame) -> str:
    from googleapiclient.http import MediaIoBaseUpload

    # ê°™ì€ ì´ë¦„ì´ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸, ì—†ìœ¼ë©´ ìƒì„±
    q = f"name = '{name}' and '{folder_id}' in parents and trashed = false"
    res = service.files().list(q=q, fields="files(id, name)").execute()
    file_id = res.get("files", [{}])[0].get("id") if res.get("files") else None

    buf = io.BytesIO()
    df.to_csv(buf, index=False, encoding="utf-8-sig")
    buf.seek(0)

    media = MediaIoBaseUpload(buf, mimetype="text/csv", resumable=False)
    if file_id:
        service.files().update(fileId=file_id, media_body=media).execute()
        return file_id
    else:
        meta = {"name": name, "parents": [folder_id], "mimeType": "text/csv"}
        created = service.files().create(body=meta, media_body=media, fields="id").execute()
        return created["id"]

def drive_download_csv(service, folder_id: str, name: str) -> Optional[pd.DataFrame]:
    q = f"name = '{name}' and '{folder_id}' in parents and trashed = false"
    res = service.files().list(q=q, fields="files(id, name)").execute()
    files = res.get("files", [])
    if not files:
        return None
    file_id = files[0]["id"]

    req = service.files().get_media(fileId=file_id)
    fh = io.BytesIO()
    from googleapiclient.http import MediaIoBaseDownload
    downloader = MediaIoBaseDownload(fh, req)
    done = False
    while not done:
        status, done = downloader.next_chunk()

    fh.seek(0)
    return pd.read_csv(fh)

# ==== Slack ====
def slack_post(text: str):
    url = os.getenv("SLACK_WEBHOOK_URL")
    if not url:
        print("[ê²½ê³ ] SLACK_WEBHOOK_URLì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë©”ì‹œì§€ë¥¼ ì¶œë ¥ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        print(text)
        return
    r = requests.post(url, json={"text": text}, timeout=15)
    if r.status_code >= 300:
        print("[Slack ì‹¤íŒ¨]", r.status_code, r.text)

# ==== ë¹„êµ/ì„¹ì…˜ ê³„ì‚° ====
def to_dataframe(products: List[Product], date_str: str) -> pd.DataFrame:
    rows = []
    for p in products:
        rows.append({
            "date": date_str,
            "rank": p.rank,
            "brand": p.brand,
            "product_name": p.title,
            "price": p.price,                 # ìˆ«ì (ë‹¬ëŸ¬)
            "orig_price": p.orig_price,       # ìˆ«ì
            "discount_percent": p.discount_percent,  # ì •ìˆ˜(ë²„ë¦¼)
            "url": p.url,
            "otuk": False if p.rank is not None else True,  # êµ­ë‚´ ë²„ì „ê³¼ ë™ì¼í•œ ì»¬ëŸ¼ ìœ ì§€
        })
    return pd.DataFrame(rows)

def line_move(name_link: str, prev_rank: Optional[int], curr_rank: Optional[int]) -> Tuple[str, int]:
    """
    í¬ë§· ë¼ì¸ê³¼ ì´ë™í­(ì ˆëŒ€ê°’) ë°˜í™˜
    """
    if prev_rank is None and curr_rank is not None:
        return f"- {name_link} NEW â†’ {curr_rank}ìœ„", 99999
    if curr_rank is None and prev_rank is not None:
        return f"- {name_link} {prev_rank}ìœ„ â†’ OUT", 99999
    if prev_rank is None or curr_rank is None:
        return f"- {name_link}", 0

    delta = prev_rank - curr_rank
    if delta > 0:
        return f"- {name_link} {prev_rank}ìœ„ â†’ {curr_rank}ìœ„ (â†‘{delta})", delta
    elif delta < 0:
        return f"- {name_link} {prev_rank}ìœ„ â†’ {curr_rank}ìœ„ (â†“{abs(delta)})", abs(delta)
    else:
        return f"- {name_link} {prev_rank}ìœ„ â†’ {curr_rank}ìœ„ (ë³€ë™ì—†ìŒ)", 0

def build_sections(df_today: pd.DataFrame, df_prev: Optional[pd.DataFrame]) -> Dict[str, List[str]]:
    """
    ì„¹ì…˜ë³„ ë¼ì¸ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    """
    sections = {"top10": [], "rising": [], "newcomers": [], "falling": [], "outs": [], "inout_count": 0}

    # ì •ë ¬/í‚¤
    df_t = df_today.copy()
    df_t["key"] = df_t["url"]  # ê³ ìœ í‚¤ë¡œ ë§í¬ ì‚¬ìš© (êµ­ë‚´ ë²„ì „ê³¼ ë™ì¼ ì „ëµ)
    df_t.set_index("key", inplace=True)

    if df_prev is not None and len(df_prev):
        df_p = df_prev.copy()
        df_p["key"] = df_p["url"]
        df_p.set_index("key", inplace=True)
    else:
        df_p = pd.DataFrame(columns=df_t.columns)

    # ---- TOP 10 ----
    top10 = df_t.dropna(subset=["rank"]).sort_values("rank").head(10)
    for _, r in top10.iterrows():
        name_only = remove_brand_from_title(r["product_name"], r.get("brand", ""))
        name_link = f"<{r['url']}|{slack_escape(name_only)}>"
        price_txt = fmt_currency_usd(r["price"])
        dc = r.get("discount_percent")
        tail = f" (â†“{int(dc)}%)" if pd.notnull(dc) else ""
        sections["top10"].append(f"{int(r['rank'])}. {name_link} â€” {price_txt}{tail}")

    # ---- ì§‘í•©/ë­í¬ ì •ë³´ ì¤€ë¹„ ----
    # Top30ë§Œ ë¹„êµ ëŒ€ìƒ
    t30 = df_t[(df_t["rank"].notna()) & (df_t["rank"] <= 30)].copy()
    p30 = df_p[(df_p["rank"].notna()) & (df_p["rank"] <= 30)].copy()

    # ê³µí†µ / ì‹ ê·œ / ì•„ì›ƒ íŒë³„
    common_keys = set(t30.index).intersection(set(p30.index))
    new_keys = set(t30.index) - set(p30.index)  # ì „ì¼ Top30 ë°– â†’ ì˜¤ëŠ˜ Top30
    out_keys = set(p30.index) - set(t30.index)  # ì „ì¼ Top30 â†’ ì˜¤ëŠ˜ Top30 ë°–(or ë¯¸ë“±ì¥)

    # ---- ê¸‰ìƒìŠ¹ (ê³µí†µ ì¤‘ ê°œì„ í­ > 0, ìƒìœ„ 3) ----
    rising_candidates = []
    for k in common_keys:
        prev_rank = int(p30.loc[k, "rank"])
        curr_rank = int(t30.loc[k, "rank"])
        imp = prev_rank - curr_rank
        if imp > 0:
            name_only = remove_brand_from_title(t30.loc[k, "product_name"], t30.loc[k].get("brand", ""))
            name_link = f"<{t30.loc[k, 'url']}|{slack_escape(name_only)}>"
            line, _ = line_move(name_link, prev_rank, curr_rank)
            rising_candidates.append((imp, curr_rank, prev_rank, slack_escape(name_only), line))
    # ì •ë ¬: ê°œì„ í­ desc â†’ ì˜¤ëŠ˜ìˆœìœ„ asc â†’ ì „ì¼ìˆœìœ„ asc â†’ ì œí’ˆëª… ê°€ë‚˜ë‹¤
    rising_candidates.sort(key=lambda x: (-x[0], x[1], x[2], x[3]))
    for entry in rising_candidates[:3]:
        sections["rising"].append(entry[-1])

    # ---- ë‰´ë­ì»¤ (ì „ì¼ Top30 ë°– â†’ ì˜¤ëŠ˜ Top30 ì§„ì…), ì˜¤ëŠ˜ìˆœìœ„ asc, ìµœëŒ€ 3 ----
    newcomers = []
    for k in new_keys:
        curr_rank = int(t30.loc[k, "rank"])
        name_only = remove_brand_from_title(t30.loc[k, "product_name"], t30.loc[k].get("brand", ""))
        name_link = f"<{t30.loc[k, 'url']}|{slack_escape(name_only)}>"
        newcomers.append((curr_rank, f"- {name_link} NEW â†’ {curr_rank}ìœ„"))
    newcomers.sort(key=lambda x: x[0])
    for _, line in newcomers[:3]:
        sections["newcomers"].append(line)

    # ---- ê¸‰í•˜ë½ (ê³µí†µ ì¤‘ í•˜ë½í­ > 0, ë‚´ë¦¼ì°¨ìˆœ ìƒìœ„ 5) + OUT ê°™ì´ í‘œê¸° ----
    falling_candidates = []
    for k in common_keys:
        prev_rank = int(p30.loc[k, "rank"])
        curr_rank = int(t30.loc[k, "rank"])
        drop = curr_rank - prev_rank
        if drop > 0:
            name_only = remove_brand_from_title(t30.loc[k, "product_name"], t30.loc[k].get("brand", ""))
            name_link = f"<{t30.loc[k, 'url']}|{slack_escape(name_only)}>"
            line, _ = line_move(name_link, prev_rank, curr_rank)
            falling_candidates.append((drop, curr_rank, prev_rank, slack_escape(name_only), line))
    falling_candidates.sort(key=lambda x: (-x[0], x[1], x[2], x[3]))
    for entry in falling_candidates[:5]:
        sections["falling"].append(entry[-1])

    # OUT (ì „ì¼ Top30 â†’ ì˜¤ëŠ˜ Top30 ë°–)
    for k in sorted(list(out_keys)):
        prev_rank = int(p30.loc[k, "rank"])
        name_only = remove_brand_from_title(p30.loc[k, "product_name"], p30.loc[k].get("brand", ""))
        name_link = f"<{p30.loc[k, 'url']}|{slack_escape(name_only)}>"
        line, _ = line_move(name_link, prev_rank, None)
        sections["outs"].append(line)

    # ---- ë­í¬ ì¸&ì•„ì›ƒ ê°œìˆ˜ ----
    sections["inout_count"] = len(new_keys) + len(out_keys)
    return sections

def build_slack_message(date_str: str, sections: Dict[str, List[str]]) -> str:
    parts = []
    parts.append(f"*ì˜¬ë¦¬ë¸Œì˜ ê¸€ë¡œë²Œëª° ë­í‚¹ â€” {date_str}*")
    parts.append("")
    # TOP 10
    parts.append("*TOP 10*")
    if sections["top10"]:
        parts += [f"{line}" for line in sections["top10"]]
    else:
        parts.append("- ë°ì´í„° ì—†ìŒ")

    # ê¸‰ìƒìŠ¹
    parts.append("")
    parts.append("*ğŸ”¥ ê¸‰ìƒìŠ¹*")
    if sections["rising"]:
        parts += sections["rising"]
    else:
        parts.append("- í•´ë‹¹ ì—†ìŒ")

    # ë‰´ë­ì»¤
    parts.append("")
    parts.append("*ğŸ†• ë‰´ë­ì»¤*")
    if sections["newcomers"]:
        parts += sections["newcomers"]
    else:
        parts.append("- í•´ë‹¹ ì—†ìŒ")

    # ê¸‰í•˜ë½ (5ê°œ) + OUT
    parts.append("")
    parts.append("*ğŸ“‰ ê¸‰í•˜ë½*")
    if sections["falling"]:
        parts += sections["falling"]
    else:
        parts.append("- í•´ë‹¹ ì—†ìŒ")
    # OUT í•¨ê»˜ í‘œê¸°
    for line in sections.get("outs", []):
        parts.append(line)

    # ë­í¬ ì¸&ì•„ì›ƒ
    parts.append("")
    parts.append("*ğŸ”„ ë­í¬ ì¸&ì•„ì›ƒ*")
    parts.append(f"{sections.get('inout_count', 0)}ê°œì˜ ì œí’ˆì´ ì¸&ì•„ì›ƒ ë˜ì—ˆìŠµë‹ˆë‹¤.")

    return "\n".join(parts)

def main():
    date_str = today_kst_str()
    ymd_yesterday = yesterday_kst_str()
    file_today = build_filename(date_str)
    file_yesterday = build_filename(ymd_yesterday)

    print("ìˆ˜ì§‘ ì‹œì‘:", BEST_URL)
    products = fetch_products()
    print(f"ìˆ˜ì§‘ ì™„ë£Œ: {len(products)}ê°œ")

    if len(products) < 10:
        raise RuntimeError("ì œí’ˆ ì¹´ë“œê°€ ë„ˆë¬´ ì ê²Œ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤. ì…€ë ‰í„° ì ê²€ í•„ìš”")

    df_today = to_dataframe(products, date_str)

    # CSV ë¡œì»¬ ì €ì¥ (ì›Œí¬í”Œë¡œ ë¡œê·¸ í™•ì¸ìš©)
    os.makedirs("data", exist_ok=True)
    local_path = os.path.join("data", file_today)
    df_today.to_csv(local_path, index=False, encoding="utf-8-sig")
    print("ë¡œì»¬ ì €ì¥:", local_path)

    # Google Drive ì—…ë¡œë“œ ë° ì „ì¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
    drive_folder = os.getenv("GDRIVE_FOLDER_ID", "").strip()
    df_prev = None
    if drive_folder:
        try:
            svc = build_drive_service()
            drive_upload_csv(svc, drive_folder, file_today, df_today)
            print("Google Drive ì—…ë¡œë“œ ì™„ë£Œ:", file_today)

            df_prev = drive_download_csv(svc, drive_folder, file_yesterday)
            if df_prev is not None:
                print("ì „ì¼ CSV ë‹¤ìš´ë¡œë“œ ì„±ê³µ:", file_yesterday)
            else:
                print("ì „ì¼ CSV ë¯¸ë°œê²¬:", file_yesterday)
        except Exception as e:
            print("Google Drive ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜:", e)
            traceback.print_exc()
    else:
        print("[ê²½ê³ ] GDRIVE_FOLDER_ID ë¯¸ì„¤ì • â†’ ë“œë¼ì´ë¸Œ ì—…ë¡œë“œ/ì „ì¼ ë¹„êµ ê±´ë„ˆëœ€")

    sections = build_sections(df_today, df_prev)
    message = build_slack_message(date_str, sections)

    slack_post(message)
    print("Slack ì „ì†¡ ì™„ë£Œ")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print("[ì˜¤ë¥˜ ë°œìƒ]", e)
        traceback.print_exc()
        # ì‹¤íŒ¨ ì‹œì—ë„ Slackì— ê°„ë‹¨íˆ ì•Œë¦¼ (ì„ íƒ)
        try:
            slack_post(f"*ì˜¬ë¦¬ë¸Œì˜ ê¸€ë¡œë²Œëª° ë­í‚¹ ìë™í™” ì‹¤íŒ¨*\n```\n{e}\n```")
        except:
            pass
        raise
