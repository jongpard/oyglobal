# -*- coding: utf-8 -*-
"""
ì˜¬ë¦¬ë¸Œì˜ ê¸€ë¡œë²Œëª° ë² ìŠ¤íŠ¸ì…€ëŸ¬ ë­í‚¹ ìˆ˜ì§‘/ë¹„êµ/ì•Œë¦¼ (USD)
- ë°ì´í„° ì†ŒìŠ¤: https://global.oliveyoung.com/display/page/best-seller?target=pillsTab1Nav1
- HTTP â†’ ì‹¤íŒ¨/ë¶€ì¡± ì‹œ Playwright
- íŒŒì¼ëª…: ì˜¬ë¦¬ë¸Œì˜ê¸€ë¡œë²Œ_ë­í‚¹_YYYY-MM-DD.csv (KST)
- ì „ì¼ CSV ë¹„êµ(Top30 ê¸°ì¤€) â†’ Slack ì•Œë¦¼
"""
import os, re, io, math, json, pytz, traceback
import datetime as dt
from dataclasses import dataclass
from typing import List, Dict, Optional, Tuple

import requests
import pandas as pd
from bs4 import BeautifulSoup

BEST_URL = "https://global.oliveyoung.com/display/page/best-seller?target=pillsTab1Nav1"
KST = pytz.timezone("Asia/Seoul")

# ---------- ê³µí†µ ìœ í‹¸ ----------
def now_kst(): return dt.datetime.now(KST)
def today_kst_str(): return now_kst().strftime("%Y-%m-%d")
def yesterday_kst_str(): return (now_kst() - dt.timedelta(days=1)).strftime("%Y-%m-%d")
def build_filename(d): return f"ì˜¬ë¦¬ë¸Œì˜ê¸€ë¡œë²Œ_ë­í‚¹_{d}.csv"

def to_float(s):
    if not s: return None
    m = re.findall(r"[\d]+(?:\.[\d]+)?", str(s))
    return float(m[0]) if m else None

def extract_percent_floor(orig_price, sale_price, percent_text):
    if percent_text:
        n = to_float(percent_text)
        if n is not None: return int(n // 1)
    if orig_price and sale_price and orig_price > 0:
        pct = (1 - (sale_price / orig_price)) * 100.0
        return max(0, int(pct // 1))
    return None

def clean_text(s): return re.sub(r"\s+", " ", (s or "")).strip()

def remove_brand_from_title(title, brand):
    t, b = clean_text(title), clean_text(brand)
    if not b: return t
    for pat in [
        rf"^\[?\s*{re.escape(b)}\s*\]?\s*[-â€“â€”:|]*\s*",
        rf"^\(?\s*{re.escape(b)}\s*\)?\s*[-â€“â€”:|]*\s*",
    ]:
        t2 = re.sub(pat, "", t, flags=re.I)
        if t2 != t: return t2.strip()
    return t

def slack_escape(s): return s.replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")
def fmt_currency_usd(v): return f"${(v or 0):,.2f}"

@dataclass
class Product:
    rank: Optional[int]
    brand: str
    title: str
    price: Optional[float]
    orig_price: Optional[float]
    discount_percent: Optional[int]
    url: str

# ---------- HTML íŒŒì„œ ----------
def parse_cards_from_html(html: str) -> List[Product]:
    soup = BeautifulSoup(html, "lxml")
    item_selectors = [
        "ul.tab_cont_list li",
        "ul.best_list li",
        "ul#bestSellerContent li",
        "li.prod_item",
        "ul li",
        "div.prod_area",
        "div.product_item, div.item"  # ì—¬ìœ  ì…€ë ‰í„°
    ]
    name_selectors = [".product_name", ".prod_name", ".name", ".tit", ".tx_name", ".item_name", "a[title]"]
    brand_selectors = [".brand", ".brand_name", ".tx_brand", ".brandName"]
    link_selectors  = ["a.prod_link", "a.link", "a.detail_link", "a"]
    price_selectors = [".price .num", ".sale_price", ".discount_price", ".final_price", ".price", ".value"]
    orig_price_selectors = [".orig_price", ".normal_price", ".consumer", ".strike", ".was"]
    percent_selectors = [".percent", ".dc", ".discount_rate", ".rate"]

    def pick_text(el, sels):
        for sel in sels:
            node = el.select_one(sel)
            if node:
                t = clean_text(node.get_text(" ", strip=True))
                if t: return t
        return ""

    def pick_link(el, sels):
        for sel in sels:
            a = el.select_one(sel)
            if a and a.has_attr("href"):
                href = a["href"].strip()
                if href and not href.startswith("javascript"): return href
        return ""

    found = []
    for sel in item_selectors:
        found = soup.select(sel)
        if len(found) >= 10: break

    items: List[Product] = []
    for idx, li in enumerate(found, start=1):
        title = pick_text(li, name_selectors)
        brand = pick_text(li, brand_selectors)
        link  = pick_link(li, link_selectors)
        sale  = to_float(pick_text(li, price_selectors))
        orig  = to_float(pick_text(li, orig_price_selectors))
        pct   = extract_percent_floor(orig, sale, pick_text(li, percent_selectors))

        if link and link.startswith("/"):
            link = "https://global.oliveyoung.com" + link
        if title and link:
            items.append(Product(idx, brand, title, sale, orig, pct, link))
    return items

# ---------- 1) HTTP ì‹œë„ ----------
def fetch_by_http() -> List[Product]:
    hdrs = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept-Language": "en-US,en;q=0.9",
        "Cache-Control": "no-cache",
        "Pragma": "no-cache",
    }
    r = requests.get(BEST_URL, headers=hdrs, timeout=25)
    r.raise_for_status()
    return parse_cards_from_html(r.text)

# ---------- 2) Playwright í´ë°± (ê°•í™”) ----------
def fetch_by_playwright() -> List[Product]:
    from playwright.sync_api import sync_playwright

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = p.chromium.launch_persistent_context(
            user_data_dir="/tmp/oyg",
            headless=True,
            locale="en-US",
            timezone_id="America/Los_Angeles",
            geolocation={"latitude": 37.7749, "longitude": -122.4194},  # US
            permissions=["geolocation"],
            user_agent=("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                        "AppleWebKit/537.36 (KHTML, like Gecko) "
                        "Chrome/120.0.0.0 Safari/537.36"),
            extra_http_headers={"Accept-Language": "en-US,en;q=0.9"},
        )

        page = context.pages[0] if context.pages else context.new_page()

        # ì§€ì—­/í†µí™” ê°•ì œ(ì¶”ì • í‚¤ í¬í•¨) â€” ì¡´ì¬í•˜ì§€ ì•Šì•„ë„ ë¬´í•´
        page.add_init_script("""
            try {
              localStorage.setItem('country', 'US');
              localStorage.setItem('oy-country', 'US');
              localStorage.setItem('currency', 'USD');
              localStorage.setItem('oy-currency', 'USD');
              localStorage.setItem('locale', 'en-US');
            } catch(e){}
        """)
        # ì¿ í‚¤ë„ ë¯¸ë¦¬ ì„¸íŒ…(ìˆìœ¼ë©´ ì‚¬ìš©)
        try:
            context.add_cookies([
                {"name": "country", "value": "US", "domain": "global.oliveyoung.com", "path": "/"},
                {"name": "currency", "value": "USD", "domain": "global.oliveyoung.com", "path": "/"},
            ])
        except: pass

        page.goto(BEST_URL, wait_until="domcontentloaded", timeout=60_000)
        try: page.wait_for_load_state("networkidle", timeout=30_000)
        except: pass

        # ì¿ í‚¤/ë™ì˜/íŒì—… ë‹«ê¸°
        for sel in [
            "button#onetrust-accept-btn-handler",
            "button:has-text('Accept All')",
            "button:has-text('Accept')",
            "button:has-text('ë™ì˜')",
            "button:has-text('í™•ì¸')",
            "[aria-label='Close']",
            "button:has-text('Donâ€™t open this window')",
        ]:
            try: page.locator(sel).first.click(timeout=1500)
            except: pass

        # íƒ­ ê°•ì œ: Global / Top Orders / All / USA
        for txt in ["Global", "Top Orders", "All", "USA", "United States"]:
            try:
                page.get_by_text(txt, exact=False).first.click(timeout=1500)
                page.wait_for_timeout(500)
            except: pass

        # ì§€ì—° ë¡œë”© ìœ ë„
        try:
            for _ in range(10):
                page.mouse.wheel(0, 2200)
                page.wait_for_timeout(700)
            page.mouse.wheel(0, -8000)
            page.wait_for_timeout(700)
        except: pass

        # 1ì°¨: ë¸Œë¼ìš°ì € DOMì—ì„œ ê¸ê¸°
        data = page.evaluate("""
            () => {
              const pick = (el, sels) => {
                for (const s of sels) {
                  const x = el.querySelector(s);
                  if (x) { const t=(x.textContent||'').replace(/\\s+/g,' ').trim(); if(t) return t; }
                }
                return '';
              };
              const pickLink = (el, sels) => {
                for (const s of sels) {
                  const a = el.querySelector(s);
                  if (a && a.href && !a.href.startsWith('javascript')) return a.href;
                }
                return '';
              };
              const itemSelectors = [
                'ul.tab_cont_list li','ul#bestSellerContent li','ul.best_list li',
                'li.prod_item','ul li','div.prod_area','div.product_item','div.item'
              ];
              const nameSelectors = ['.product_name','.prod_name','.name','.tit','.tx_name','.item_name','a[title]'];
              const brandSelectors = ['.brand','.brand_name','.tx_brand','.brandName'];
              const linkSelectors  = ['a.prod_link','a.link','a.detail_link','a'];
              const priceSelectors = ['.price .num','.sale_price','.discount_price','.final_price','.price','.value'];
              const origSelectors  = ['.orig_price','.normal_price','.consumer','.strike','.was'];
              const percentSelectors = ['.percent','.dc','.discount_rate','.rate'];

              let nodes = [];
              for (const s of itemSelectors) {
                const found = Array.from(document.querySelectorAll(s));
                if (found.length >= 10) { nodes = found; break; }
                if (!nodes.length && found.length) nodes = found;
              }
              return nodes.map((el, idx) => {
                const title = pick(el, nameSelectors);
                const brand = pick(el, brandSelectors);
                const link  = pickLink(el, linkSelectors);
                const price = pick(el, priceSelectors);
                const orig  = pick(el, origSelectors);
                const pct   = pick(el, percentSelectors);
                return {rank: idx+1, title, brand, link, price, orig, pct};
              }).filter(x => x.title && x.link);
            }
        """)

        products: List[Product] = []
        if not data or len(data) < 10:
            # 2ì°¨: HTML í†µì§¸ë¡œ ë°›ì•„ BeautifulSoup ì¬íŒŒì‹±
            html = page.content()
            products = parse_cards_from_html(html)

        if (not products) and data:
            # DOM ë°ì´í„°ë¡œ êµ¬ì„±
            for row in data:
                sale = to_float(row.get("price"))
                orig = to_float(row.get("orig"))
                pct  = extract_percent_floor(orig, sale, row.get("pct"))
                products.append(Product(
                    rank=row.get("rank"),
                    brand=clean_text(row.get("brand")),
                    title=clean_text(row.get("title")),
                    price=sale, orig_price=orig, discount_percent=pct,
                    url=row.get("link"),
                ))

        # 3ì°¨(ë§ˆì§€ë§‰): í˜ì´ì§€ ì „ì—­ JSON ì¶”ì¶œ ì‹œë„ (Next/Nuxt ë“±)
        if len(products) < 10:
            try:
                j = page.evaluate("() => (window.__NEXT_DATA__ || window.__NUXT__ || window.__APOLLO_STATE__ || null)")
                if j:
                    s = json.dumps(j)
                    # ë§¤ìš° ëŠìŠ¨í•œ ì¶”ì¶œ (ë§í¬/ì´ë¦„/ê°€ê²©)
                    candidates = re.findall(r'"(name|title)":"(.*?)".*?"(sale|price)\\w*":\\s*("?\\d+(?:\\.\\d+)?")', s)
                    # ì´ ë£¨íŠ¸ëŠ” ì‚¬ì´íŠ¸ êµ¬ì¡° íŒŒì•… ì „ ì„ì‹œ ë°±ìŠ¤í†±. ì¶©ë¶„ì¹˜ ì•Šìœ¼ë©´ ë¬´ì‹œ.
            except: 
                pass

        context.close(); browser.close()
        return products

def fetch_products() -> List[Product]:
    # 1) HTTP
    try:
        items = fetch_by_http()
        if len(items) >= 10:
            return items
    except Exception as e:
        print("[HTTP] ì‹¤íŒ¨/ë¶€ì¡± â†’ Playwright:", e)
    # 2) Playwright
    return fetch_by_playwright()

# ---------- Drive / Slack / ë¹„êµ ë¡œì§ (ë™ì¼) ----------
from googleapiclient.discovery import build
def build_drive_service():
    from google.oauth2 import service_account
    from google.oauth2.credentials import Credentials
    sa_json = os.getenv("GDRIVE_SERVICE_ACCOUNT_JSON", "").strip()
    scopes = ["https://www.googleapis.com/auth/drive"]
    if sa_json:
        info = json.loads(sa_json)
        creds = service_account.Credentials.from_service_account_info(info, scopes=scopes)
    else:
        cid = os.getenv("GOOGLE_CLIENT_ID")
        csec = os.getenv("GOOGLE_CLIENT_SECRET")
        rtk  = os.getenv("GOOGLE_REFRESH_TOKEN")
        if not (cid and csec and rtk):
            raise RuntimeError("Google Drive ìê²©ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        creds = Credentials(None, refresh_token=rtk, token_uri="https://oauth2.googleapis.com/token",
                            client_id=cid, client_secret=csec, scopes=scopes)
    return build("drive", "v3", credentials=creds, cache_discovery=False)

def drive_upload_csv(service, folder_id, name, df):
    from googleapiclient.http import MediaIoBaseUpload
    q = f"name = '{name}' and '{folder_id}' in parents and trashed = false"
    res = service.files().list(q=q, fields="files(id,name)").execute()
    file_id = res.get("files", [{}])[0].get("id") if res.get("files") else None
    buf = io.BytesIO(); df.to_csv(buf, index=False, encoding="utf-8-sig"); buf.seek(0)
    media = MediaIoBaseUpload(buf, mimetype="text/csv", resumable=False)
    if file_id:
        service.files().update(fileId=file_id, media_body=media).execute(); return file_id
    else:
        meta = {"name": name, "parents": [folder_id], "mimeType": "text/csv"}
        return service.files().create(body=meta, media_body=media, fields="id").execute()["id"]

def drive_download_csv(service, folder_id, name):
    q = f"name = '{name}' and '{folder_id}' in parents and trashed = false"
    res = service.files().list(q=q, fields="files(id,name)").execute()
    files = res.get("files", [])
    if not files: return None
    file_id = files[0]["id"]
    req = service.files().get_media(fileId=file_id)
    fh = io.BytesIO()
    from googleapiclient.http import MediaIoBaseDownload
    dl = MediaIoBaseDownload(fh, req); done=False
    while not done: _, done = dl.next_chunk()
    fh.seek(0); return pd.read_csv(fh)

def slack_post(text):
    url = os.getenv("SLACK_WEBHOOK_URL")
    if not url:
        print("[ê²½ê³ ] SLACK_WEBHOOK_URL ë¯¸ì„¤ì • â†’ ì½˜ì†” ì¶œë ¥")
        print(text); return
    r = requests.post(url, json={"text": text}, timeout=15)
    if r.status_code >= 300:
        print("[Slack ì‹¤íŒ¨]", r.status_code, r.text)

def to_dataframe(products: List[Product], date_str: str) -> pd.DataFrame:
    return pd.DataFrame([{
        "date": date_str, "rank": p.rank, "brand": p.brand, "product_name": p.title,
        "price": p.price, "orig_price": p.orig_price, "discount_percent": p.discount_percent,
        "url": p.url, "otuk": False if p.rank is not None else True
    } for p in products])

def line_move(name_link, prev_rank, curr_rank):
    if prev_rank is None and curr_rank is not None: return f"- {name_link} NEW â†’ {curr_rank}ìœ„", 99999
    if curr_rank is None and prev_rank is not None: return f"- {name_link} {prev_rank}ìœ„ â†’ OUT", 99999
    if prev_rank is None or curr_rank is None:    return f"- {name_link}", 0
    delta = prev_rank - curr_rank
    if   delta > 0: return f"- {name_link} {prev_rank}ìœ„ â†’ {curr_rank}ìœ„ (â†‘{delta})", delta
    elif delta < 0: return f"- {name_link} {prev_rank}ìœ„ â†’ {curr_rank}ìœ„ (â†“{abs(delta)})", abs(delta)
    else:           return f"- {name_link} {prev_rank}ìœ„ â†’ {curr_rank}ìœ„ (ë³€ë™ì—†ìŒ)", 0

def build_sections(df_today, df_prev):
    S = {"top10": [], "rising": [], "newcomers": [], "falling": [], "outs": [], "inout_count": 0}
    df_t = df_today.copy(); df_t["key"] = df_t["url"]; df_t.set_index("key", inplace=True)
    if df_prev is not None and len(df_prev):
        df_p = df_prev.copy(); df_p["key"] = df_p["url"]; df_p.set_index("key", inplace=True)
    else:
        df_p = pd.DataFrame(columns=df_t.columns)

    top10 = df_t.dropna(subset=["rank"]).sort_values("rank").head(10)
    for _, r in top10.iterrows():
        name_only = remove_brand_from_title(r["product_name"], r.get("brand",""))
        name_link = f"<{r['url']}|{slack_escape(name_only)}>"
        price_txt = fmt_currency_usd(r["price"])
        dc = r.get("discount_percent"); tail = f" (â†“{int(dc)}%)" if pd.notnull(dc) else ""
        S["top10"].append(f"{int(r['rank'])}. {name_link} â€” {price_txt}{tail}")

    t30 = df_t[(df_t["rank"].notna()) & (df_t["rank"] <= 30)].copy()
    p30 = df_p[(df_p["rank"].notna()) & (df_p["rank"] <= 30)].copy()
    common = set(t30.index) & set(p30.index)
    new    = set(t30.index) - set(p30.index)
    out    = set(p30.index) - set(t30.index)

    rising = []
    for k in common:
        prev_rank, curr_rank = int(p30.loc[k,"rank"]), int(t30.loc[k,"rank"])
        imp = prev_rank - curr_rank
        if imp > 0:
            nm = remove_brand_from_title(t30.loc[k,"product_name"], t30.loc[k].get("brand",""))
            link = f"<{t30.loc[k,'url']}|{slack_escape(nm)}>"
            line,_ = line_move(link, prev_rank, curr_rank)
            rising.append((imp, curr_rank, prev_rank, slack_escape(nm), line))
    rising.sort(key=lambda x: (-x[0], x[1], x[2], x[3]))
    S["rising"] = [e[-1] for e in rising[:3]]

    newcomers = []
    for k in new:
        curr_rank = int(t30.loc[k,"rank"])
        nm = remove_brand_from_title(t30.loc[k,"product_name"], t30.loc[k].get("brand",""))
        link = f"<{t30.loc[k,'url']}|{slack_escape(nm)}>"
        newcomers.append((curr_rank, f"- {link} NEW â†’ {curr_rank}ìœ„"))
    newcomers.sort(key=lambda x: x[0])
    S["newcomers"] = [line for _, line in newcomers[:3]]

    falling = []
    for k in common:
        prev_rank, curr_rank = int(p30.loc[k,"rank"]), int(t30.loc[k,"rank"])
        drop = curr_rank - prev_rank
        if drop > 0:
            nm = remove_brand_from_title(t30.loc[k,"product_name"], t30.loc[k].get("brand",""))
            link = f"<{t30.loc[k,'url']}|{slack_escape(nm)}>"
            line,_ = line_move(link, prev_rank, curr_rank)
            falling.append((drop, curr_rank, prev_rank, slack_escape(nm), line))
    falling.sort(key=lambda x: (-x[0], x[1], x[2], x[3]))
    S["falling"] = [e[-1] for e in falling[:5]]

    for k in sorted(list(out)):
        prev_rank = int(p30.loc[k,"rank"])
        nm = remove_brand_from_title(p30.loc[k,"product_name"], p30.loc[k].get("brand",""))
        link = f"<{p30.loc[k,'url']}|{slack_escape(nm)}>"
        line,_ = line_move(link, prev_rank, None)
        S["outs"].append(line)

    S["inout_count"] = len(new) + len(out)
    return S

def build_slack_message(date_str, S):
    parts = [f"*ì˜¬ë¦¬ë¸Œì˜ ê¸€ë¡œë²Œëª° ë­í‚¹ â€” {date_str}*", "", "*TOP 10*"]
    parts += S["top10"] or ["- ë°ì´í„° ì—†ìŒ"]
    parts += ["", "*ğŸ”¥ ê¸‰ìƒìŠ¹*"] + (S["rising"] or ["- í•´ë‹¹ ì—†ìŒ"])
    parts += ["", "*ğŸ†• ë‰´ë­ì»¤*"] + (S["newcomers"] or ["- í•´ë‹¹ ì—†ìŒ"])
    parts += ["", "*ğŸ“‰ ê¸‰í•˜ë½*"] + (S["falling"] or ["- í•´ë‹¹ ì—†ìŒ"]) + S.get("outs", [])
    parts += ["", "*ğŸ”„ ë­í¬ ì¸&ì•„ì›ƒ*", f"{S.get('inout_count', 0)}ê°œì˜ ì œí’ˆì´ ì¸&ì•„ì›ƒ ë˜ì—ˆìŠµë‹ˆë‹¤."]
    return "\n".join(parts)

def slack_post_or_print(msg):
    try: slack_post(msg)
    except Exception as e: print("[Slack ì˜¤ë¥˜]", e); print(msg)

def main():
    date_str = today_kst_str()
    ymd_yesterday = yesterday_kst_str()
    file_today = build_filename(date_str)
    file_yesterday = build_filename(ymd_yesterday)

    print("ìˆ˜ì§‘ ì‹œì‘:", BEST_URL)
    items = []
    try:
        items = fetch_by_http()
        print("[HTTP] ìˆ˜ì§‘:", len(items))
    except Exception as e:
        print("[HTTP ì˜¤ë¥˜]", e)

    if len(items) < 10:
        print("[Playwright í´ë°± ì§„ì…]")
        items = fetch_by_playwright()
    print("ìˆ˜ì§‘ ì™„ë£Œ:", len(items))

    if len(items) < 10:
        raise RuntimeError("ì œí’ˆ ì¹´ë“œê°€ ë„ˆë¬´ ì ê²Œ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤. ì…€ë ‰í„°/ë Œë”ë§ ì ê²€ í•„ìš”")

    df_today = to_dataframe(items, date_str)
    os.makedirs("data", exist_ok=True)
    local_path = os.path.join("data", file_today)
    df_today.to_csv(local_path, index=False, encoding="utf-8-sig")
    print("ë¡œì»¬ ì €ì¥:", local_path)

    drive_folder = os.getenv("GDRIVE_FOLDER_ID", "").strip()
    df_prev = None
    if drive_folder:
        try:
            svc = build_drive_service()
            drive_upload_csv(svc, drive_folder, file_today, df_today)
            print("Google Drive ì—…ë¡œë“œ ì™„ë£Œ:", file_today)
            df_prev = drive_download_csv(svc, drive_folder, file_yesterday)
            print("ì „ì¼ CSV", "ì„±ê³µ" if df_prev is not None else "ë¯¸ë°œê²¬", file_yesterday)
        except Exception as e:
            print("Google Drive ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜:", e); traceback.print_exc()
    else:
        print("[ê²½ê³ ] GDRIVE_FOLDER_ID ë¯¸ì„¤ì • â†’ ë“œë¼ì´ë¸Œ ì—…ë¡œë“œ/ì „ì¼ ë¹„êµ ìƒëµ")

    S = build_sections(df_today, df_prev)
    msg = build_slack_message(date_str, S)
    slack_post_or_print(msg)
    print("Slack ì „ì†¡ ì™„ë£Œ")

if __name__ == "__main__":
    try: main()
    except Exception as e:
        print("[ì˜¤ë¥˜ ë°œìƒ]", e); traceback.print_exc()
        try: slack_post(f"*ì˜¬ë¦¬ë¸Œì˜ ê¸€ë¡œë²Œëª° ë­í‚¹ ìë™í™” ì‹¤íŒ¨*\n```\n{e}\n```")
        except: pass
        raise
